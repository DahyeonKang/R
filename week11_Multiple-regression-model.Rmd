---
title: "Multiple Regression Model"
author: "강다현"
date: '2020 11 17'
output: html_document
---

# 데이터 생성

다음과 같은 함수를 고려하자.
$$
f(x) = \sin^3(2 \pi x^2),
\quad
x \in [0, 1].
$$
위 함수를 이용해서 다음과 같은 선형회귀모형을 고려하자.
$$
Y = f(x) + \varepsilon.
$$
여기서 오차는 $\varepsilon \sim N(0, \sigma^2)$이며 이때 $\sigma = 0.1$이다.
위 회귀모형으로부터 $n = 200$개의 데이터를 생성해보자.
이때 $x$는 $[0, 1]$에서 등간격으로 생성한다.

```{r data}
rm(list = ls())
# seed 고정
set.seed(54)  # seed를 고정하지 않으면 같은 코드를 써도 항상 다른 결과가 나올 것이다.
# data generation
n = 200
x = seq(0, 1, length = n)
# true function
f = sin(2 * pi * x^2)^3
# response
sigma = 0.1  # 이렇게 변수를 생성해서 상수를 넣어주는 습관을 만들기로 하자 !
y = f + rnorm(n, sd = sigma)
# data plot
plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f, col = "blue")
```

우리의 목표는 data만 주어졌을 때, 실제 회귀선을 추정하는 것이다.
이제 추정 과정을 전개해보자.

# 단순선형회귀모형 적합
```{r slr}
lm_fit = lm(y ~ x)
summary(lm_fit)

plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f)
lines(x, lm_fit$fitted.values, col = "blue")
```

보다시피, 직선만 가지고는 위 자료를 설명하는데 한계가 있다.
따라서, 이차항에 해당하는 $x^2$을 변수에 추가해서 이차시을 적합시켜보자.

## 이차항을 추가한 이차식 적합

```{r x2}

x2 = x^2
lm_fit2 = lm(y ~ x + x2)
summary(lm_fit2)

plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f)
lines(x, lm_fit2$fitted.values, col = "blue")
```

위의 코드를 보고 알 수 있는 점은 이차항을 추가해도 실제 회귀함수를 따라가는 데에는 무리가 있다.

## lm 함수의 다른 옵션 소개

2개 이상의 설명변수가 존재할 경우, 설명변수 데이터들을 행렬화해서 lm 함수에 적용할 수 있다.

```{r matrix}
# 행렬만들기
X = cbind(x, x^2)  # 대문자 X를 회귀에서 '설계행렬'이라고 한다.
# 설계행렬을 이용해서 중회귀모형 적합
lm_fit2_matrix = lm(y ~ X)
summary(lm_fit2_matrix)

plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f)
lines(x, lm_fit2_matrix$fitted.values, col = "blue")
```

## 3차항 추가
```{r x3}
X = cbind(x, x^2, x^3)

lm_fit3 = lm(y ~ X)
summary(lm_fit3)

plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f)
lines(x, lm_fit3$fitted.values, col = "blue")
```

## 4차항 추가
```{r x4}
X = cbind(x, x^2, x^3, x^4)

lm_fit4 = lm(y ~ X)
summary(lm_fit4)

plot(x, y, col = "gray", cex = 1.2, bty = "n")
lines(x, f)
```

## for 함수를 이용한 여러 다항회귀함수 적합

for 함수를 이용해서 1차부터 10차까지의 다항회귀함수를 적합해보고자 한다. 결과는 그림으로만 확인하자.
```{r lm_for}
X = NULL  # 빈 변수 생성
for (i in 1:10)
{
   X = cbind(X, x^i)  # 이 for문 안의 핵심은 설계행렬이다. 
   lm_fit = lm(y ~ X)
   plot(x, y, col = "gray", cex = 1.2, bty = "n")
   lines(x, f)
   lines(x, lm_fit$fitted.values, col = "blue")
}
```

## 문서 정리

바로 위의 for문을 이용한 함수를 조금 더 가다듬어서 문서정리를 해보자.

```{r lm_for2, fig.height = 10, fig.width = 10}
# 그림 행렬 분할
# 그림을 그릴 때, 설정한 행렬 바둑판식으로 배열
# 그림은 행을 우선으로 채워넣음
par(mfrow = c(3, 3))
X = NULL  # 빈 변수 생성  
for (i in 1:10)
{
   X = cbind(X, x^i)   
   lm_fit = lm(y ~ X)
   plot(x, y, col = "gray", cex = 1.2, bty = "n",
        main = paste(i, "-th regression curve"))
   lines(x, f)
   lines(x, lm_fit$fitted.values, col = "blue")
}
```

## 여러 적합 결과들을 겹쳐서 확인하기
```{r plot_all, fig.height = 6, fig.width = 6}
X = NULL  # 빈 변수 생성
plot(x, y, col = "gray", cex = 1.2, bty = "n",
        main = paste("regression curve"))
lines(x, f, lwd = 2)  # 여러 회귀선이 겹치기 떄문에, 실제선의 굵기를 늘린다.(강조)
# rainbow 색깔 설정
# alpha = 색깔의 투명도
rain_color = rainbow(10, alpha = 0.2)
for (i in 1:10)
{
   X = cbind(X, x^i)  # 이 for문 안의 핵심은 설계행렬이다. 
   lm_fit = lm(y ~ X)
   lines(x, lm_fit$fitted.values, col = rain_color[i],
         lwd = 0.5)  # 하나의 그림에 여러 개의 선을 넣을 때, 얇은 선을 사용하면 더 잘 보일 수 있다.
}

```

## 최적의 모형 선택

지금까지 해당 자료에 대해서 여러 가지 다항회귀함수를 적합(추정)해보았는데 , 이 중에서 자료를 가장 잘 설명하는 추정함수가 무엇일까?


summary를 했을 때, 'Adjusted R-squared' 라는 값이 결과에 나오는데, 이는 여러가지 모형을 적합시켰을 떄 가장 적합한 모형을 선택할 수 있는 기준이 된다.

```{r adjustedR}
lm_fit = lm(y ~ x)
lm_fit_summary = summary(lm_fit)
lm_fit_summary$adj.r.squared
```

summary로 요약된 정보를 이용해서 adj.r.squred 값을 저장할 수 있다.
이 저장된 값을 잘 이용하면 많은 정보를 얻을 수 있다.

## 예제

1. adjuted R-square (수정된 결정계수, $R^2_a$) 값을 근거로 하여 1-50차 추정함수 중 가장 $R^2_a$값이 큰 모형을 선택하는 알고리즘을 구현해보자.

2. 다음과 같이 $x$축을 최고차창 차원 (1:50), $y$축을 각 모형의 $R^2_a$값을 나타내는 그림을 그려보자.
```{r adjustedR_ex}
par(mfrow = c(1, 1))  # 행렬의 바둑판식의 틀 잡기
X = NULL  # 빈 변수 생성
adj_R = NULL # 빈 변수 생성(반복문을 거치면서 벡터로 변환됨)

for (i in 1:50)
{
   X = cbind(X, x^i)  # design matrix 생성
   lm_fit = lm(y ~ X)
   sum_lm_fit = summary(lm_fit)  # summary에 다양한 값이 리스트형태로 저장됨
   adj_R[i] = sum_lm_fit$adj.r.squared
}
adj_R
```

```{r max}
# max : 최대 값을 반환
max(adj_R)
# which.max : 최대 값이 있는 위치 반환
which.max(adj_R)

plot(1:50, adj_R, type = "o", bty = "n",
     xlab = "number of predictors", ylab = "Adjusted R-squares")
```

* 가장 큰 $R^2_a$에 해당하는 위치(index)를 which.max 함수를 적절히 이용해서 찾고, 다음과 같은 메세지를 cat 함수를 이용해서 출력해볼 것
```{r cat}
index = which.max(adj_R)
cat(c("Adjusted R-squre 값이 가장 큰 모형은 =", index,  "번째 모형이다."))
```

* 그림의 효과를 극대화 하기 위해서, 앞의 1-14번째 모형은 제외하고, 15번째 모형부터 50번째 모형에 대한 $R^2_a$ 값만 그려볼 것.
* 앞에서 얻은 최적의 모형에 해당하는 위치를 아래와 같이 강조하여 표현해 볼 것.
```{r adjustedR_plot}
plot(15:50, adj_R[15:50], type = "o", bty = "n",
     xlab = "number of predictors", ylab = "Adjusted R-squares")
points(index, adj_R[index], col = "red", pch = 16, cex = 2)
abline(v = index, col = "gray", lwd = 1.5, lty = 2)  # 수직선을 그어주는 함수
```

### 최종으로 선택된 모형만 다시 적합시켜 시각화해볼 것.

앞에서 얻어진 결과, 26번째 모형, 즉 z에 대한 26차식으로 적합시킨 모형이 $R^2_a$ 기준으로 최적의 모형이다. 이에 대한 적합결과를 그리면 다음과 같다.

```{r lm_adjustedR}
X = NULL
for(i in 1:index)
   X = cbind(X, x^i)
lm_fit = lm(y ~ X)
plot(x, y, col = "gray", cex = 1.5, bty = "n",
     main = paste("best regression curve"))
lines(x, f, lwd = 2)
lines(x, lm_fit$fitted.values, col = "blue", lwd = 1.5)
legend("topright", legend = c("true curve", "best fit"),
       col = c("black", "blue"), lwd = 1, lty = 1, bty = "n")
```